global:
  config_name: preprocess_gg_aug_bbc-gvc

pipeline:

  # load the dev and test splits of the to-be-augmented CDCR dataset (this will merge them into one dataset)

  # GVC
  - python.handwritten_baseline.pipeline.data.loader.gvc_loader:
      gvc_root_dir: "resources/data/gun_violence"
      gvc_split_csv_filename: "dev.csv"
      drop_0_cluster: True
  - python.handwritten_baseline.pipeline.data.loader.gvc_loader:
      gvc_root_dir: "resources/data/gun_violence"
      gvc_split_csv_filename: "test.csv"
      drop_0_cluster: True

  # load hyperlink data, and use the 'difference' operation to remove all hyperlinks documents which are similar to
  # the datasets loaded prior
  - python.handwritten_baseline.pipeline.data.loader.hyperlinks_loader:
      page_infos: "resources/data/hypercoref/bbc.com/train/page_infos.parquet"
      tokens: "resources/data/hypercoref/bbc.com/train/tokens.parquet"
      hyperlinks: "resources/data/hypercoref/bbc.com/train/hyperlinks.parquet"

      # this here is crucial!
      combine_operation: "difference"
      combine_difference_threshold: 0.25

  - python.handwritten_baseline.pipeline.data.processing.reducer:
      event_cluster_size_interval_to_keep: [2, 10]

  - python.handwritten_baseline.pipeline.data.processing.hyperlinks_hack:
      constituency: "resources/data/hypercoref/bbc.com/constituency.hdf"
      dependency: "resources/data/hypercoref/bbc.com/dependency.hdf"

      drop_phrasal_mentions: False
      reduce_span_to_dependency_head: True

      target_num_docs_per_fake_topic: 50
      total_mentions_limit: 5000

  # add GVC here
  - python.handwritten_baseline.pipeline.data.loader.gvc_loader:
      gvc_root_dir: "resources/data/gun_violence"
      drop_0_cluster: True
      gvc_split_csv_filename: "train.csv"

  - python.handwritten_baseline.pipeline.data.processing.dataset_exporter:
      dataset_name: gg_aug_bbc-gvc
      split: train
      export_in_cattan_format_as_well: True