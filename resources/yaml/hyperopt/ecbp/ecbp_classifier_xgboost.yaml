global:
  config_name: ecbp_hyperopt_classifier_xgboost


model:
  # Two options here:
  # (1) When running hyperparameter optimization for the mention pair classifier, this should be a string with the name
  #     of the classifier to optimize. Accepted values: SVC_huber, LR, XGBoost, MLP
  # (2) When running hyperparameter optimization for the clustering step, or when training the final model, this should
  #     be a dict with the type, args and kwargs necessary to instantiate the classifier.
  classifier: "XGBoost"

  features:
    # Extractors to use. If null, all extractors will be used with an exception: Extractors listed here which have no
    # feature selected below will not be instantiated.
    extractors:

    # Features of each extractor to use. If null, all features will be used.
    selected_features:
      # following the results of 26388_2020-06-30_17-06-31
      - lemma#is-surface-form-identical
      - lemma#is-lemma-identical
      - lemma#surface-form-mlinps-distance
      - lemma#surface-form-levenshtein-distance
      - tfidf#document-similarity
      - tfidf#surrounding-sentence-similarity
      - tfidf#context-similarity
      - sentence-embedding#doc-start
      - action-phrase-embedding#action-phrase

data:
  # paths to preprocessed pickle versions of datasets
  train_data_path: ""
  eval_data_path: ""

  # ECB+: gold topics, because topics were annotated individually
  doc_partitioning: "gold_topics"

  # if true, use gold labels for advanced mention pair generation
  oracle_mention_pair_generation: true

  pairs:
    # mention pair generator settings for training
    mpg_training:
      undersample_c: 8
      neg_to_pos_pair_ratio: 8

    # mention pair generator settings for predicting
    mpg_prediction:
      undersample_c: 8
      neg_to_pos_pair_ratio: 8

hyperopt:
  # True: optimize agglomerative clustering, False: optimize mention pair classifier
  with_clustering: False

  # how many repeats to use in repeated k-fold CV
  cv_num_repeats: 4

  # These have the same behaviour as reported here [1]. Timeout is parsed with pandas. Keep n_trials empty (== None)
  # and set timeout to try as many configs as possible in the given duration.
  # [1] https://optuna.readthedocs.io/en/latest/reference/study.html#optuna.study.Study.optimize
  n_trials:
  timeout: "24 hours"

  # This setting does unfortunately not work with parallelized optimization!
#  early_stopping:
#    patience: 30
#    min_delta: 0.001